<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width">
    <title>nicholas sofroniew</title>
    <link rel="alternate" href="http://localhost:8080/feed.xml" type="application/rss+xml" title="Musings">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic|Anonymous+Pro:400,700,400italic,700italic">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700,300,100">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Raleway:100,400,200,300">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic">
    <link rel="stylesheet" href="/css/shelves.css">
    <link rel="stylesheet" href="/css/main.css">
    <link rel="icon" sizes="16x16 32x32" href="../favicon.png">
    <script src="//use.edgefonts.net/merriweather:n3,n4,n7,n9.js"></script>
  </head>
  <body>
    <div id="wrapper">
      <div id="header">
        <div class="row">
          <div class="column-2"><a href="/" class="nav-home">&lsaquo;&lsaquo;&nbsp; home</a></div>
          <div class="column-5 prefix-5">
            <div class="nav-group"><a href="/about/" class="nav nav-about">about</a><span class="divider">/  </span><a href="/projects/" class="nav nav-team nav-team-selected">projects</a></div>
          </div>
        </div>
        <div class="row">
          <div class="column-7">
            <h1 class="sub-header">recent projects</h1>
          </div>
        </div>
      </div>
      <div id="content">
        <div style="margin-top: 40px;margin-bottom: 40px" class="row">
          <!-- <p class="work-category">biology</p> -->
          <div class="work-item">
            <div class="row">
              <div class="column-8">
                <p class="work-name">2-photon random access mesoscope <p>
                <p class="work-description">During my postdoctoral research I developed a novel microscope (2-photon random access mesoscope, 2pRAM) that is capable of subcellular resolution imaging across a 5 mm diameter field of view. Complex behaviors depend on coordinated neural activity across many disparate cortical areas; however, simultaneously recording neural activity in multiple cortical areas with single neuron resolution and high speed had not been possible with existing technology. Using the 2pRAM, we now routinely track activity in thousands of neurons across sensory, motor, and parietal cortex simultaneously in mice navigating virtual mazes, allowing us to follow neural coding during learning of complex behavior. To disseminate the microscope to the community, we ran a workshop for 14 groups teaching them how to build the microscope. In addition, we licensed the technology to a company, Thorlabs, that is beginning to sell the microscope. </p>
                <p class="work-link-group"><a href="/projects/pdfs/sofroniew-2016-elife.pdf" target="_blank" class="work-link">paper</a><a href="https://www.thorlabs.com/newgrouppage9.cfm?objectgroup_id=10646" target="_blank" class="work-link">thorlabs</a>
                </p>
              </div>
              <div class="column-3 prefix-1"><img src="/projects/photos/mesoscope-map.png" class="work-photo"></div>
            </div>
          </div>
          <div class="work-item">
            <div class="row">
              <div class="column-8">
                <p class="work-name">data analysis</p>
                <p class="work-description">During my postdoctoral research, I worked to make analysis code and data from large-scale neural recordings more shareable with the community. The large data sets (>50GB) generated during two-photon calcium imaging experiments pose significant data analysis challenges and require a new set of efficient analysis tools. I have worked to integrate Thunder, an analysis library written in Python based on the distributed computation engine Spark, into a command line based analysis pipeline. Thunder leverages distributed computer clusters to perform operations in parallel, causing analyses that used to take days to finish in minutes. This efficient analysis pipeline now allows us to close the loop between experiments, theory, and analysis. I have worked to make our software tools and data available to the community using GitHub and Binder, a service made by the Freeman lab that facilitates sharing analysis code in an executable and interactive manner. Using Binder I have enabled readers to reproduce analyses and figures from two of my recent papers in an interactive notebook environment.
                </p>
                <p class="work-link-group"><a href="/projects/pdfs/freeman-2014-nature-methods.pdf" target="_blank" class="work-link">paper</a><a href="https://github.com/sofroniewn/mesoscope" target="_blank" class="work-link">repo</a><a href="https://github.com/sofroniewn/tactile-coding" target="_blank" class="work-link">repo</a><a href="https://github.com/sofroniewn/2pRAM-paper" target="_blank" class="work-link">repo</a>
                </p>
              </div>
              <div class="column-3 prefix-1"><img src="/projects/photos/paper-bindered.png" class="work-photo"></div>
            </div>
          </div>
          <div class="work-item">
            <div class="row">
              <div class="column-8">
                <p class="work-name">tactile virtual reality</p>
                <p class="work-description">During my PhD I developed an ethologically relevant and highly quantitative tactile virtual reality system for behavioral and neurophysiological experiments in mice. In this system, a mouse is free to run on a large air supported ball while its head is fixed in place. The rotation of the ball is coupled to the location of physical walls, controlled by motors, creating winding virtual corridors for the mice to explore. Mice use their whiskers to determine the position of the walls and guide their locomotion appropriately without training. As the mice are head-fixed, their neural activity can be recorded with 2-photon microscopy and acute penetrations of silicon probe electrodes, or precisely manipulated with targeted optogenetic interventions. Using these techniques, I examined the role of the somatosensory cortex in whisker-guided locomotion. We discovered a rich neural representation of nearby walls, with individual neurons tuned to specific wall distances and directions of wall motion. More recently, I have been developing a foraging task in tactile virtual reality that requires mice to plan their decisions based on previous actions and an internal model of the environment.
                </p>
                <p class="work-link-group"><a href="/projects/pdfs/sofroniew-2014-jneuro.pdf" target="_blank" class="work-link">paper</a><a href="/projects/pdfs/sofroniew-2015-elife.pdf" target="_blank" class="work-link">paper</a><a href="/projects/pdfs/sofroniew-2015-whisking.pdf" target="_blank" class="work-link">paper</a><a href="https://www.youtube.com/watch?v=wt-tzNvZ3Gk" target="_blank" class="work-link">bbc video</a><a href="https://www.technologyreview.com/s/541891/how-brain-scientists-outsmart-their-lab-mice" target="_blank" class="work-link">MIT tech review</a>
                </p>
              </div>
              <div class="column-3 prefix-1"><img src="/projects/photos/VR-rig.png" class="work-photo"></div>
            </div>
          </div>
        </div>
      <div id="footer">
        <div style="max-width: 1010px" class="row no-select">
          <div style="text-align: right" class="column-3 prefix-9"><a href="http://twitter.com/sofroniewn" target="_blank" class="round"><span class="round-inside">t</span></a><a href="http://github.com/sofroniewn" target="_blank" class="round"><span class="round-inside">g</span></a><a href="mailto:sofroniewn-at-gmail.org" class="round"><span class="round-inside">m</span></a></div>
        </div>
      </div>
    </div>
  </body>
</html>
